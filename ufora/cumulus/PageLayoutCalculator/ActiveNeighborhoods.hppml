/***************************************************************************
    Copyright 2015 Ufora Inc.

    Licensed under the Apache License, Version 2.0 (the "License");
    you may not use this file except in compliance with the License.
    You may obtain a copy of the License at

        http://www.apache.org/licenses/LICENSE-2.0

    Unless required by applicable law or agreed to in writing, software
    distributed under the License is distributed on an "AS IS" BASIS,
    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    See the License for the specific language governing permissions and
    limitations under the License.
****************************************************************************/
#pragma once

#include "../../core/math/Smallest.hpp"
#include "../../core/threading/Queue.hpp"
#include "../ComputationId.hppml"
#include "../../FORA/TypedFora/ABI/BigVectorPageLayout.hppml"
#include "../SystemwideComputationScheduler/ThreadGroup.hppml"
#include "../SystemwideComputationScheduler/ThreadGroupStateUpdate.hppml"

#include "WorkingSet.hppml"
#include "ThreadGroupPages.hppml"
#include "MachinePageAllocation.hppml"
#include "TaskAllocator.hppml"


namespace Cumulus {

namespace PageLayoutCalculator {

/****************************

ActiveNeighborhoods

Maintains a history of "load level" on active pages, a current page layout,
and a target page layout.

*****************************/

class ActiveNeighborhoods {
public:
	ActiveNeighborhoods(int64_t maxBytesToLoadAtOnce) :
			mTotalMemoryAvailable(0),
			mTaskAllocator(
				maxBytesToLoadAtOnce
				)
		{
		}

	ImmutableTreeMap<MachineId, int64_t> allocationForTask(hash_type dataTaskId)
		{
		return mTaskAllocator.accumulatorBytecounts(TaskAllocatorId::ForDataTask(dataTaskId));
		}

	void createTemporaryPageGroup(hash_type guid, ImmutableTreeSet<Fora::PageId> pages)
		{
		std::set<Fora::PageId> pagesSet;

		for (auto p: pages)
			if (!isPageDroppedAcrossSystem(p))
				pagesSet.insert(p);

		mTaskAllocator.setTaskProperties(TaskAllocatorId::ForTemporary(guid), 1, pagesSet);
		mTemporaryPageGroupPages.insert(guid, pagesSet);
		}

	void dropTemporaryPageGroup(hash_type guid)
		{
		mTaskAllocator.dropTask(TaskAllocatorId::ForTemporary(guid));
		mTemporaryPageGroupPages.dropKey(guid);
		}

	void handleThreadUpdate(ThreadGroupStateUpdate update)
		{
		LOG_DEBUG << "Handling update " << update;

		if (mThreadGroupLastUpdate.find(update.group()) != mThreadGroupLastUpdate.end())
			mThreadGroupLastUpdate2[update.group()] = mThreadGroupLastUpdate[update.group()];
		mThreadGroupLastUpdate[update.group()] = update;

		if (!update.isFinished())
			{
			mThreadGroupPages.setGroupThreads(update.group(), update.totalLeafThreadsBelow());

			for (auto page: update.newPages())
				if (!isPageDroppedAcrossSystem(page))
					mThreadGroupPages.add(update.group(), page);

			for (auto page: update.droppedPages())
				if (!isPageDroppedAcrossSystem(page))
					mThreadGroupPages.drop(update.group(), page);

			TaskAllocatorId task = TaskAllocatorId::ForThreadGroup(update.group().hash());

			if (mTaskAllocator.hasTask(task))
				{
				for (auto page: update.newPages())
					if (!isPageDroppedAcrossSystem(page))
						{
						mWorkingSet.touch(page);
						mTaskAllocator.taskAddPage(task, page);
						}

				for (auto page: update.droppedPages())
					if (!isPageDroppedAcrossSystem(page))
						mTaskAllocator.taskDropPage(task, page);

				mTaskAllocator.setTaskThreads(task, update.totalLeafThreadsBelow());
				}
			}
		else
			{
			mThreadGroupPages.groupFinished(update.group());
			mThreadGroupLastUpdate.erase(update.group());
			mThreadGroupLastUpdate2.erase(update.group());
			mTaskAllocator.dropTask(TaskAllocatorId::ForThreadGroup(update.group().hash()));
			mThreadGroupToTaskId.discard(update.group());
			}
		}

	bool threadGroupWantsSplit(ThreadGroup group)
		{
		if (mThreadGroupLastUpdate.find(group) == mThreadGroupLastUpdate.end())
			return false;

		//if we reset recently and have not sent an update, no reason to reset
		if (mThreadGroupLastUpdate2.find(group) != mThreadGroupLastUpdate.end() &&
				mThreadGroupLastUpdate[group].timesReset() == mThreadGroupLastUpdate2[group].timesReset() + 1)
			return false;

		auto it = mThreadGroupLastSplit.find(group);
		if (it != mThreadGroupLastSplit.end() && it->second.timesReset() == mThreadGroupLastUpdate[group].timesReset())
			return false;

		if (mThreadGroupLastUpdate[group].totalBytesOfPagesReferenced() > maxGroupBytecount())
			mThreadGroupLastSplit[group] = mThreadGroupLastUpdate[group];

		return mThreadGroupLastUpdate[group].totalBytesOfPagesReferenced() > maxGroupBytecount();
		}

	int64_t maxGroupBytecount() const
		{
		return mMaxGroupBytecount;
		}

	void allocateDataTask(hash_type dataTaskId, int64_t bytecount, MachineId preferredMachine)
		{
		mTaskAllocator.setAccumulatorProperties(
			TaskAllocatorId::ForDataTask(dataTaskId),
			bytecount,
			preferredMachine
			);
		}

	void dropDataTask(hash_type dataTaskId)
		{
		mTaskAllocator.dropTask(TaskAllocatorId::ForDataTask(dataTaskId));
		}

	bool isPageDroppedAcrossSystem(Fora::PageId page) const
		{
		return mPagesDroppedAcrossEntireSystem.find(page) != mPagesDroppedAcrossEntireSystem.end();
		}

	int64_t totalUsableBytes() const
		{
		return mTotalMemoryAvailable;
		}

	int64_t bytesAddedToMachine(MachineId inMachine)
		{
		return mTaskAllocator.getPageAllocation().bytesAddedToMachine(inMachine);
		}

	ImmutableTreeSet<Fora::PageId> getDesiredMachineContents(const MachineId& inMachine)
		{
		return ImmutableTreeSet<Fora::PageId>(
			mTaskAllocator.getPageAllocation().getTargetPages().getValues(inMachine)
			);
		}

	void setMachineMemory(const MachineId& inMachine, uint64_t inMemoryLimit)
		{
		return setMachineUsableMemory(inMachine, inMemoryLimit);
		}

	const MachinePageAllocation& pageAllocation() const
		{
		return mTaskAllocator.getPageAllocation();
		}

	/**************** current state of the system ***********************/
	void addMachine(MachineId machine)
		{
		lassert(!hasMachine(machine));

		mMachines.insert(machine);
		mMachineSizes[machine] = 0;
		}

	ImmutableTreeSet<Fora::PageId> getAllPagesInActiveSet() const
		{
		return ImmutableTreeSet<Fora::PageId>(mWorkingSet.getPages());
		}

	bool hasMachine(MachineId machine) const
		{
		return mMachineSizes.find(machine) != mMachineSizes.end();
		}

	int64_t getMachineUsableMemory(MachineId machine) const
		{
		auto it = mMachineSizes.find(machine);

		if (it == mMachineSizes.end())
			return 0;

		return it->second;
		}

	int64_t getMachineCoreCount(MachineId machine) const
		{
		auto it = mMachineCoreCount.find(machine);

		if (it == mMachineCoreCount.end())
			return 0;

		return it->second;
		}

	void setMachineUsableMemory(MachineId machine, int64_t bytesAvailable)
		{
		setMachineUsableMemory(machine, bytesAvailable, getMachineCoreCount(machine));
		}

	void setMachineCoreCount(MachineId machine, int64_t cores)
		{
		setMachineUsableMemory(machine, getMachineUsableMemory(machine), cores);
		}

	void setMachineUsableMemory(MachineId machine, int64_t bytesAvailable, int64_t coreCount)
		{
		lassert(hasMachine(machine));

		mTotalMemoryAvailable += bytesAvailable - mMachineSizes[machine];
		mMachineSizes[machine] = bytesAvailable;
		mMachineCoreCount[machine] = coreCount;

		mMaxGroupBytecount = mTotalMemoryAvailable / mMachineSizes.size() / 10;

		mWorkingSet.setWorkingSetSize(mTotalMemoryAvailable * .5);

		mThreadGroupPages.setMaxActiveBytes(mTotalMemoryAvailable * .4);

		mTaskAllocator.setMachineStats(machine, bytesAvailable, coreCount);
		}

	template<class T>
	void dumpThreadGroupsToLog(T& log)
		{
		log << "ThreadGroups:\n";

		std::map<hash_type, std::map<std::string, ThreadGroup> > groupsByPrefix;

		for (auto& grp: mThreadGroupLastUpdate)
			groupsByPrefix[grp.first.rootComputation().hash()][grp.first.prefix()] = grp.first;

		int64_t totalActiveBytes = 0;

		for (auto machine: mMachines)
			log << prettyPrintString(machine).substr(1,3) << "|";

		log << "Active?   |";
		log << "Underalloc? |";
		log << "AcMem|";
		log << "CuMem|";
		log << "Loads |";
		log << "Time |";
		log << "Thr  |";
		log << "Prefix   ";
		log << "\n";

		for (auto& hashAndGroups: groupsByPrefix)
			{
			log << hashAndGroups.first << ":\n";

			for (auto& prefixAndGrp: hashAndGroups.second)
				{
				ThreadGroup group = prefixAndGrp.second;
				TaskAllocatorId task = TaskAllocatorId::ForThreadGroup(group.hash());

				int64_t bytes = 0;
				for (auto& p: mThreadGroupPages.getThreadGroupPages().getValues(group))
					bytes += p.bytecount();

				if (bytes || mThreadGroupLastUpdate[group].totalLeafThreadsBelow())
					{
					std::ostringstream line;

					for (auto machine: mMachines)
						if (mTaskAllocator.loadingTasks().contains(task, machine))
							line << " + |";
							else
						if (mTaskAllocator.runningTasks().contains(task, machine))
							line << "***|";
						else
							{
							if (bytes == 0)
								line << "   |";
							else
								{
								float frac = (bytes - mTaskAllocator.bytesToScheduleTaskOnMachine(task, machine)) / float(bytes);
								if (frac < .25)
									line << "   |";
									else
								if (frac < .5)
									line << ".  |";
									else
								if (frac < .75)
									line << ".. |";
								else
									line << "...|";
								}
							}

					if (mTaskAllocator.hasTask(task))
						line << "          ";
					else
						line << "<inactive>";
					line << "|";

					if (mTaskAllocator.taskIsUnderallocated(task))
						line << "++          ";
					else
						line << "            ";
					line << "|";

					if (bytes / 1024 / 1024 > 0)
						line << std::setw(5) << int(bytes / 1024 / 1024);
					else
						line << "     ";
					line << "|";

					if (mThreadGroupLastUpdate[group].totalBytesOfPagesEverReferenced() / 1024 / 1024 > 0)
						line << std::setw(5) << int(mThreadGroupLastUpdate[group].totalBytesOfPagesEverReferenced() / 1024 / 1024);
					else
						line << "     ";
					line << "|";

					if (mTaskAllocator.bytesLoadedByTaskOverLifetime(task) / 1024 / 1024)
						line << std::setw(6) << int(mTaskAllocator.bytesLoadedByTaskOverLifetime(task) / 1024 / 1024);
					else
						line << "      ";
					line << "|";

					if (int(mThreadGroupLastUpdate[group].timeElapsed()) > 0)
						line << std::setw(5) << std::setprecision(2) << (mThreadGroupLastUpdate[group].timeElapsed());
					else
						line << "     ";
					line << "|";

					if (mTaskAllocator.taskThreads(task) > 0)
						line << std::setw(5) << int(mTaskAllocator.taskThreads(task));
					else
						line << "     ";
					line << "|";

					line << "  " << prefixAndGrp.first << "\n";

					log << line.str();
					}

				totalActiveBytes += bytes;
				}
			}

		log << "TemporaryPageGroups:\n";
		for (auto& tempGroupAndPages: mTemporaryPageGroupPages.getKeysToValues())
			{
			log << tempGroupAndPages.first << " needs "
				<< tempGroupAndPages.second << " pages."
				<< mTaskAllocator.loadingTasks().getValues(TaskAllocatorId::ForTemporary(tempGroupAndPages.first)).size()
				<< " loading and "
				<< mTaskAllocator.runningTasks().getValues(TaskAllocatorId::ForTemporary(tempGroupAndPages.first)).size()
				<< " running.";

			log << "\n";
			}

		log << "\ntotal WS Bytes = " << mWorkingSet.bytesInSet() / 1024 / 1024.0 << " MB"
			<< " / " << mWorkingSet.maxBytesInSet() / 1024 / 1024.0 << " MB"
			<< "\n";
		log << "\ntotalActiveBytes = " << totalActiveBytes / 1024 / 1024.0 << " MB\n";
		log << "\nThreadGroupBytes: "
			<< mThreadGroupPages.activeBytes() / 1024 / 1024.0 << " MB of "
			<< " / " << mThreadGroupPages.maxActiveBytes() / 1024 / 1024.0 << " MB\n";

		for (auto m: mTaskAllocator.machines())
			log << m << ": "
				<< (mTaskAllocator.machineCouldHandleMoreWork(m) ? "underloaded. ": "fully loaded. ")
				<< mTaskAllocator.machineAllocatedCoreCount(m) << " cores allocated. "
				<< mTaskAllocator.loadingTasks().getKeys(m).size() << " tasks loading. "
				<< mTaskAllocator.runningTasks().getKeys(m).size() << " tasks running. "
				<< mTaskAllocator.machineMaxMemory(m) / 1024 / 1024 << " MB max. "
				<< mTaskAllocator.bytesAllocatedToAccumulatorTasksOnMachine(m) / 1024 / 1024 << " MB of data tasks. "
				<< mTaskAllocator.bytesInAllTasks(m) / 1024 / 1024 << " MB in other tasks."
				<< "\n"
				;

		for (auto taskAndRequested: mTaskAllocator.getAccumulatorTargetBytecounts())
			log << "DataTask: " << taskAndRequested.first.guid() << " wants " 
				<< taskAndRequested.second / 1024 / 1024.0 << " MB. "
				<< mTaskAllocator.bytesAllocatedToAccumulatorTask(taskAndRequested.first) / 1024 / 1024.0 << " MB allocated.\n";
		}

	void addPage(MachineId machine, Fora::PageId page)
		{
		if (isPageDroppedAcrossSystem(page))
			return;

		mTaskAllocator.pageAdded(machine, page);
		}

	void pageDroppedAcrossEntireSystem(Fora::PageId page)
		{
		LOG_INFO << "ActiveNeighborhoods acknowledging page " << page << " dropped everywhere.";

		mTemporaryPageGroupPages.dropValue(page);
		mPagesDroppedAcrossEntireSystem.insert(page);
		mTaskAllocator.pageDroppedAcrossEntireSystem(page);
		mThreadGroupPages.pageDroppedAcrossEntireSystem(page);
		mWorkingSet.pageDroppedAcrossEntireSystem(page);
		}

	void dropPageFromMachine(const Fora::PageId& inPage, const MachineId& inMachine)
		{
		if (isPageDroppedAcrossSystem(inPage))
			return;

		mTaskAllocator.pageDropped(inMachine, inPage);
		}


	ImmutableTreeVector<ThreadGroup> getThreadGroupsActiveOnMachine(MachineId machine) const
		{
		const auto& threadGroupTaskIds = mTaskAllocator.runningTasks().getKeys(machine);

		ImmutableTreeVector<ThreadGroup> res;

		for (auto taskId: threadGroupTaskIds)
			for (auto group: mThreadGroupToTaskId.getKeys(taskId))
				res = res + group;

		return res;
		}

	void rebalance()
		{
		std::set<ThreadGroup> activated, deactivated;

		mThreadGroupPages.extractActivatedOrDeactivatedThreadGroups(activated, deactivated);

		for (auto group: activated)
			{
			//touch every page this group touches in the AWS
			for (auto page: mThreadGroupPages.getThreadGroupPages().getValues(group))
				if (!isPageDroppedAcrossSystem(page))
					mWorkingSet.touch(page);

			TaskAllocatorId taskId = TaskAllocatorId::ForThreadGroup(group.hash());

			mTaskAllocator.setTaskProperties(
				taskId,
				mThreadGroupLastUpdate[group].totalLeafThreadsBelow(),
				mThreadGroupPages.getThreadGroupPages().getValues(group)
				);

			mThreadGroupToTaskId.set(group, taskId);
			}

		for (auto& tempGroupAndPages: mTemporaryPageGroupPages.getKeysToValues())
			for (auto p: tempGroupAndPages.second)
				mWorkingSet.touch(p);

		for (auto group: deactivated)
			mTaskAllocator.dropTask(TaskAllocatorId::ForThreadGroup(group.hash()));

		//now walk over the AWS dropped pages and check that none of them is part of an active group
		//as we are trying to keep them all active there.
		std::set<Fora::PageId> added, pruned;
		do {
			mWorkingSet.extractChanges(added, pruned);

			for (auto p: added)
				mTaskAllocator.addToWorkingSet(p);

			for (auto p: pruned)
				mTaskAllocator.dropFromWorkingSet(p);

			for (auto p: pruned)
				if (mThreadGroupPages.getActivePages().hasValue(p))
					mWorkingSet.touch(p);
			} while (pruned.size());

		mTaskAllocator.rebalance();
		}

	const std::set<MachineId>& machines() const
		{
		return mMachines;
		}

private:
	std::map<hash_type, ImmutableTreeMap<MachineId, int64_t> > mRealizedDataTaskLayout;

	std::map<hash_type, ImmutableTreeMap<MachineId, int64_t> > mDesiredDataTaskLayout;

	std::map<MachineId, int64_t> mRealizedDataTaskBytes;

	std::map<MachineId, int64_t> mDesiredDataTaskBytes;

	TaskAllocator mTaskAllocator;

	WorkingSet mWorkingSet;

	int64_t mMaxGroupBytecount;

	map<MachineId, int64_t> mMachineSizes;

	map<MachineId, int64_t> mMachineCoreCount;

	set<MachineId> mMachines;

	std::set<Fora::PageId> mPagesDroppedAcrossEntireSystem;

	std::map<ThreadGroup, ThreadGroupStateUpdate> mThreadGroupLastUpdate;

	std::map<ThreadGroup, ThreadGroupStateUpdate> mThreadGroupLastUpdate2;

	std::map<ThreadGroup, ThreadGroupStateUpdate> mThreadGroupLastSplit;

	ThreadGroupPages mThreadGroupPages;

	MapWithIndex<ThreadGroup, TaskAllocatorId> mThreadGroupToTaskId;

	TwoWaySetMap<hash_type, Fora::PageId> mTemporaryPageGroupPages;

	int64_t mTotalMemoryAvailable;
};

}
}
