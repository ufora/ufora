//
// Generated by NVIDIA NVVM Compiler then manipulated manually
// Compiler built on Fri Aug  1 05:29:38 2014 (1406860178)
// Cuda compilation tools, release 6.5, V6.5.14
//

.version 4.1
.target sm_20
.address_size 64

.func (.reg .f64 output) logf64 (.reg .f64 input)
{
    .reg .pred  %p<9>;
    .reg .s32   %r<26>;
    .reg .f64   %fd<58>;
	

    {
    .reg .b32 %temp;
    mov.b64     {%temp, %r22}, input;
    }
    setp.gt.f64 %p1, input, 0d0000000000000000;
    setp.lt.s32 %p2, %r22, 2146435072;
    and.pred    %p3, %p1, %p2;
    @%p3 bra    BB0_6;

    abs.f64     %fd9, input;
    setp.gtu.f64    %p4, %fd9, 0d7FF0000000000000;
    @%p4 bra    BB0_5;

    setp.neu.f64    %p5, input, 0d0000000000000000;
    @%p5 bra    BB0_4;

    mov.f64     output, 0dFFF0000000000000;
    bra.uni     BB0_12;

BB0_4:
    setp.eq.f64 %p6, input, 0d7FF0000000000000;
    selp.f64    output, input, 0dFFF8000000000000, %p6;
    bra.uni     BB0_12;

BB0_5:
    add.f64     output, input, input;
    bra.uni     BB0_12;

BB0_6:
    {
    .reg .b32 %temp;
    mov.b64     {%r23, %temp}, input;
    }
    setp.lt.s32 %p7, %r22, 1048576;
    @%p7 bra    BB0_8;

    mov.u32     %r24, -1023;
    bra.uni     BB0_9;

BB0_8:
    mul.f64     %fd11, input, 0d4350000000000000;
    {
    .reg .b32 %temp;
    mov.b64     {%temp, %r22}, %fd11;
    }
    {
    .reg .b32 %temp;
    mov.b64     {%r23, %temp}, %fd11;
    }
    mov.u32     %r24, -1077;

BB0_9:
    shr.u32     %r13, %r22, 20;
    add.s32     %r25, %r24, %r13;
    and.b32     %r14, %r22, -2146435073;
    or.b32      %r15, %r14, 1072693248;
    mov.b64     %fd56, {%r23, %r15};
    setp.lt.s32 %p8, %r15, 1073127583;
    @%p8 bra    BB0_11;

    {
    .reg .b32 %temp;
    mov.b64     {%r16, %temp}, %fd56;
    }
    {
    .reg .b32 %temp;
    mov.b64     {%temp, %r17}, %fd56;
    }
    add.s32     %r18, %r17, -1048576;
    mov.b64     %fd56, {%r16, %r18};
    add.s32     %r25, %r25, 1;

BB0_11:
    add.f64     %fd13, %fd56, 0d3FF0000000000000;
    mov.f64     %fd14, 0d3FF0000000000000;
    // inline asm
    rcp.approx.ftz.f64 %fd12,%fd13;
    // inline asm %fd1
    neg.f64     %fd15, %fd13;
    fma.rn.f64  %fd16, %fd15, %fd12, %fd14;
    fma.rn.f64  %fd17, %fd16, %fd16, %fd16;
    fma.rn.f64  %fd18, %fd17, %fd12, %fd12;
    add.f64     %fd19, %fd56, 0dBFF0000000000000;
    mul.f64     %fd20, %fd19, %fd18;
    fma.rn.f64  %fd21, %fd19, %fd18, %fd20;
    mul.f64     %fd22, %fd21, %fd21;
    mov.f64     %fd23, 0d3ED0EE258B7A8B04;
    mov.f64     %fd24, 0d3EB1380B3AE80F1E;
    fma.rn.f64  %fd25, %fd24, %fd22, %fd23;
    mov.f64     %fd26, 0d3EF3B2669F02676F;
    fma.rn.f64  %fd27, %fd25, %fd22, %fd26;
    mov.f64     %fd28, 0d3F1745CBA9AB0956;
    fma.rn.f64  %fd29, %fd27, %fd22, %fd28;
    mov.f64     %fd30, 0d3F3C71C72D1B5154;
    fma.rn.f64  %fd31, %fd29, %fd22, %fd30;
    mov.f64     %fd32, 0d3F624924923BE72D;
    fma.rn.f64  %fd33, %fd31, %fd22, %fd32;
    mov.f64     %fd34, 0d3F8999999999A3C4;
    fma.rn.f64  %fd35, %fd33, %fd22, %fd34;
    mov.f64     %fd36, 0d3FB5555555555554;
    fma.rn.f64  %fd37, %fd35, %fd22, %fd36;
    sub.f64     %fd38, %fd19, %fd21;
    add.f64     %fd39, %fd38, %fd38;
    neg.f64     %fd40, %fd21;
    fma.rn.f64  %fd41, %fd40, %fd19, %fd39;
    mul.f64     %fd42, %fd18, %fd41;
    mul.f64     %fd43, %fd37, %fd22;
    fma.rn.f64  %fd44, %fd43, %fd21, %fd42;
    xor.b32     %r19, %r25, -2147483648;
    mov.u32     %r20, -2147483648;
    mov.u32     %r21, 1127219200;
    mov.b64     %fd45, {%r19, %r21};
    mov.b64     %fd46, {%r20, %r21};
    sub.f64     %fd47, %fd45, %fd46;
    mov.f64     %fd48, 0d3FE62E42FEFA39EF;
    fma.rn.f64  %fd49, %fd47, %fd48, %fd21;
    neg.f64     %fd50, %fd47;
    fma.rn.f64  %fd51, %fd50, %fd48, %fd49;
    sub.f64     %fd52, %fd51, %fd21;
    sub.f64     %fd53, %fd44, %fd52;
    mov.f64     %fd54, 0d3C7ABC9E3B39803F;
    fma.rn.f64  %fd55, %fd47, %fd54, %fd53;
    add.f64     output, %fd49, %fd55;

BB0_12:
    ret;
}


.func (.reg .f32 output) logf32 (.reg .f32 input)
{
	.reg .pred 	%p<6>;
	.reg .s32 	%r<5>;
	.reg .f32 	%f<33>;


	setp.gt.f32	%p1, input, 0f00000000;
	setp.lt.f32	%p2, input, 0f7F800000;
	and.pred  	%p3, %p1, %p2;
	@%p3 bra 	BB0_2;

	lg2.approx.f32 	output, input;
	bra.uni 	BB0_3;

BB0_2:
	setp.lt.f32	%p4, input, 0f00800000;
	mul.f32 	%f7, input, 0f4B800000;
	selp.f32	%f8, %f7, input, %p4;
	selp.f32	%f9, 0fC3170000, 0fC2FE0000, %p4;
	mov.b32 	 %r1, %f8;
	and.b32  	%r2, %r1, 8388607;
	or.b32  	%r3, %r2, 1065353216;
	mov.b32 	 %f10, %r3;
	shr.u32 	%r4, %r1, 23;
	cvt.rn.f32.u32	%f11, %r4;
	add.f32 	%f12, %f9, %f11;
	setp.gt.f32	%p5, %f10, 0f3FAE147B;
	mul.f32 	%f13, %f10, 0f3F000000;
	add.f32 	%f14, %f12, 0f3F800000;
	selp.f32	%f15, %f13, %f10, %p5;
	selp.f32	%f16, %f14, %f12, %p5;
	add.f32 	%f6, %f15, 0f3F800000;
	add.f32 	%f17, %f15, 0fBF800000;
	// inline asm
	rcp.approx.ftz.f32 %f5,%f6;
	// inline asm
	neg.f32 	%f18, %f17;
	mul.f32 	%f19, %f17, %f18;
	mul.rn.f32 	%f20, %f5, %f19;
	add.rn.f32 	%f21, %f17, %f20;
	mul.f32 	%f22, %f21, %f21;
	mov.f32 	%f23, 0f3C4C6A36;
	mov.f32 	%f24, 0f3B1E94E6;
	fma.rn.f32 	%f25, %f24, %f22, %f23;
	mov.f32 	%f26, 0f3DAAAB1A;
	fma.rn.f32 	%f27, %f25, %f22, %f26;
	mul.f32 	%f28, %f27, %f22;
	fma.rn.f32 	%f29, %f28, %f21, %f20;
	add.f32 	%f30, %f29, %f17;
	mov.f32 	%f31, 0f3F317218;
	fma.rn.f32 	output, %f16, %f31, %f30;

BB0_3:
	ret;
}

