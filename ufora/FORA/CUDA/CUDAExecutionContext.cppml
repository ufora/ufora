/***************************************************************************
    Copyright 2015 Ufora Inc.

    Licensed under the Apache License, Version 2.0 (the "License");
    you may not use this file except in compliance with the License.
    You may obtain a copy of the License at

        http://www.apache.org/licenses/LICENSE-2.0

    Unless required by applicable law or agreed to in writing, software
    distributed under the License is distributed on an "AS IS" BASIS,
    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    See the License for the specific language governing permissions and
    limitations under the License.
****************************************************************************/

#include "CUDAExecutionContext.hppml"
#include "NativeCFGToPTX.hppml"
#include "../Core/Alignment.hpp"
#include "../Core/AlignmentManager.hpp"
#include "../Core/ExecutionContext.hppml"
#include "../Core/MemoryPool.hpp"
#include "../Core/ImplValContainerUtilities.hppml"
#include "../Native/NativeCFGTransforms/ConvertForGPUExecution.hppml"
#include "../Native/NativeCFGTransforms/Transforms.hppml"
#include "../Native/NativeCode.hppml"
#include "../Runtime.hppml"
#include "../TypedFora/JitCompiler/StaticInliner.hppml"
#include "../TypedFora/Converter.hppml"
#include "../TypedFora/ABI/SlottedForaValueArrayAppend.hppml"
#include "../TypedFora/ABI/VectorRecord.hpp"
#include "../TypedFora/ABI/VectorHandle.hpp"
#include "../TypedFora/TypedFora.hppml"
#include "../Reasoner/SimpleForwardReasoner.hppml"
#include "../VectorDataManager/PageletTree.hppml"
#include "../../core/Logging.hpp"
#include "../../core/threading/Queue.hpp"

#include <cuda.h>
#include <cuda_runtime_api.h>

using TypedFora::Abi::VectorHandle;
using TypedFora::Abi::VectorRecord;
using Fora::Interpreter::ExecutionContext;

constexpr uint8_t GPU_EXCEPTION_INDEX = 0xFF;

class CUDAExecutionContextInternalState {
public:

	CUDAExecutionContextInternalState() : mDeviceCount(0)
		{
		cudaDriverInitializer();
		cudaDeviceInitializer();
		}


	//initialize CUDA. single threaded, only happens once
	static	void	cudaDriverInitializer()
		{
		static boost::recursive_mutex	mMutex;

		static bool isInitialized = false;

		if (!isInitialized)
			{
			boost::recursive_mutex::scoped_lock	lock(mMutex);
			if (!isInitialized)
				{
				//initialize the driver
				CUresult error = cuInit(0);
				throwOnError("Initialize cuda", error);
				isInitialized = true;
				}
			}
		}

	static void throwOnError(std::string op, CUresult error)
		{
		if (error != CUDA_SUCCESS)
			{
			const char* ptr = 0;
			cuGetErrorString(error, &ptr);
			throw UnableToConvertToPTX("Couldn't " + op + ": " + std::string(ptr ? ptr : "") +
				". code=" + boost::lexical_cast<string>(error));
			}
		}

	void	cudaDeviceInitializer()
		{
		CUresult error = cuDeviceGetCount(&mDeviceCount);
		throwOnError("Get a device count", error);

		lassert_dump(mDeviceCount, "there are no CUDA-enabled devices");

		//take the first device
		mCuDevices.resize(mDeviceCount);
		mCuContexts.resize(mDeviceCount);

		for (long devID = 0; devID < mDeviceCount; devID++)
			{
			int major, minor;
			constexpr int DEVICE_NAME_MAX_LENGTH = 256;
			char deviceName[DEVICE_NAME_MAX_LENGTH];

			cuDeviceComputeCapability(&major, &minor, devID);
			cuDeviceGetName(deviceName, DEVICE_NAME_MAX_LENGTH, devID);

			LOG_INFO 	<< "CUDA Using Device " << devID
						<< ": \"" << deviceName
						<< "\" with Compute " << major << "." << minor
						<< "capability.\n"
						;

			// pick up device with zero ordinal (default, or devID)
			error = cuDeviceGet(&mCuDevices[devID], devID);

			lassert(error == CUDA_SUCCESS);

			// Create context
			error = cuCtxCreate(&mCuContexts[devID], 0, mCuDevices[devID]);
			lassert(error == CUDA_SUCCESS);
			}

		for (auto c: mCuContexts)
			mUnusedContexts.write(c);
		}

	string addLineNumbers(const string& str)
		{
		ostringstream outStr;

		uint32_t lastIx = 0;
		uint32_t lineNo = 1;

		for (long k = 0; k < str.size();k++)
			if (str[k] == '\n' || k == str.size() - 1)
				{
				outStr << lineNo << "   ";
				lineNo++;

				outStr << str.substr(lastIx, k - lastIx) << "\n";
				lastIx = k+1;
				}

		return outStr.str();
		}
	void addCudaModule(	const string& inModuleName,
						const string& inFunctionName,
						const string& inPTXCode)
		{
		for (auto context: mCuContexts)
			{
			cuCtxSetCurrent(context);

			const unsigned int jitNumOptions = 5;
			CUjit_option *jitOptions = new CUjit_option[jitNumOptions];
			void **jitOptVals = new void*[jitNumOptions];

			int bufferSize = 1024;
			char	jitLogBuffer[1024];
			char	jitErrorBuffer[1024];

			jitOptions[0] = CU_JIT_INFO_LOG_BUFFER_SIZE_BYTES;
			jitOptVals[0] = (void *)(size_t)bufferSize;
			jitOptions[1] = CU_JIT_INFO_LOG_BUFFER;
			jitOptVals[1] = jitLogBuffer;

			jitOptions[2] = CU_JIT_ERROR_LOG_BUFFER_SIZE_BYTES;
			jitOptVals[2] = (void*)(size_t)bufferSize;
			jitOptions[3] = CU_JIT_ERROR_LOG_BUFFER;
			jitOptVals[3] = jitErrorBuffer;

			jitOptions[4] = CU_JIT_MAX_REGISTERS;
			int jitRegCount = 32;
			jitOptVals[4] = (void *)(size_t)jitRegCount;

			CUmodule	cuModule;

			CUresult error = cuModuleLoadDataEx(&cuModule, inPTXCode.c_str(),
							jitNumOptions, jitOptions, (void **)jitOptVals);

			if (error != CUDA_SUCCESS)
				{
				LOG_WARN << "CUDA FAILED TO COMPILE: " +
												string(jitLogBuffer) + "\n" +
												string(jitErrorBuffer) + "\n" +
												"\n\nPTX code = \n\n" +
												addLineNumbers(inPTXCode);

				lassert_dump(false, "CUDA FAILED TO COMPILE: " +
												string(jitLogBuffer) + "\n" +
												string(jitErrorBuffer) + "\n" +
												"\n\nPTX code = \n\n" +
												addLineNumbers(inPTXCode)
												);
				}

			CUfunction cuFun;

			// Get function handle from module
			error = cuModuleGetFunction(&cuFun, cuModule,
												inFunctionName.c_str());
			if (error != CUDA_SUCCESS)
				lassert_dump(false, "PTX code didn't define function " <<
											inFunctionName);

				{
				boost::mutex::scoped_lock lock(mMutex);
				mCuFunctionsByName[make_pair(context, inModuleName)] = cuFun;
				}
			}

		}


	CUfunction functionNameForModule(CUcontext context, std::string inModuleName)
		{
		boost::mutex::scoped_lock lock(mMutex);
		return mCuFunctionsByName[make_pair(context, inModuleName)];
		}

	void executeFunction(	const std::string&				inModuleName,
							uint8_t*						inCudaClosureData,
							uword_t							inN,
							uint8_t*						inCudaSourceData,
							std::vector<uint8_t*>&			outCudaDestDataVectors,
							uint8_t*						outCudaIndexData
							)
		{
		lassert(outCudaDestDataVectors.size() > 0);

		uword_t outputTypesCount = outCudaDestDataVectors.size();
		CUcontext contextToUse = mUnusedContexts.get();

		try {
			cuCtxSetCurrent(contextToUse);
			std::vector<CUdeviceptr> d_destData(outputTypesCount);

			CUresult error;

			// Grid/Block configuration
			int threadsPerBlock = 1024;
			int blocksPerGrid   = (inN + threadsPerBlock - 1) / threadsPerBlock;

			uword_t kk = 0;
			uword_t extraArgs = 2; // inN and sourceDataPtr
			if (inCudaClosureData)
				++extraArgs;
			if (outCudaIndexData)
				++extraArgs;
			uword_t argCount = outputTypesCount + extraArgs;
			void **args = (void**) malloc(sizeof(void*) * (argCount));
			if (inCudaClosureData)
				args[kk++] = &inCudaClosureData;
			args[kk++] = &inN;
			args[kk++] = &inCudaSourceData;
			if (outCudaIndexData)
				args[kk++] = &outCudaIndexData;
			for (uword_t k = 0; k < outputTypesCount; ++k)
				{
				lassert(kk < argCount);
				args[kk++] = &outCudaDestDataVectors[k];
				}
			lassert(kk == argCount);

			// Launch the CUDA kernel
			error = cuLaunchKernel( functionNameForModule(contextToUse, inModuleName),
									blocksPerGrid, 1, 1,
									threadsPerBlock, 1, 1,
									0,
									NULL, args, NULL);
			throwOnError("Launch Kernel: ", error);
			lassert_dump(error == CUDA_SUCCESS, "UNKNOWN CUDA ERROR");
			cudaDeviceSynchronize();
			//free cuda memory
			free(args);

			mUnusedContexts.write(contextToUse);
			}
		catch(...)
			{
			mUnusedContexts.write(contextToUse);
			// FIXME: we are not freeing the cuda allocated memory properly
			throw;
			}
		}
private:
	int mDeviceCount;

	Queue<CUcontext> mUnusedContexts;

	std::vector<CUdevice> mCuDevices;

	std::vector<CUcontext> mCuContexts;

	boost::mutex mMutex;

	map<pair<CUcontext, string>, CUfunction> mCuFunctionsByName;
};

CUDAExecutionContext::CUDAExecutionContext() :
		mCUDAState(new CUDAExecutionContextInternalState())
	{
	}

void	CUDAExecutionContext::define(
						const std::string& inKernelName,
						const NativeCFG& inCFG,
						const Type& inInputType,
						const std::vector<Type>& inOutputTypes
						)
	{
	boost::recursive_mutex::scoped_lock lock(mMutex);

	// TODO:: assert that inCFG.returnTypes() match inOutputTypes
	string 	ptxDefinition =
		computePTXVectorApplyKernelFromNativeCFG(inCFG, inKernelName);

	mCUDAState->addCudaModule(inKernelName, inKernelName, ptxDefinition);

	mNativeKernelsByName[inKernelName] = inCFG;
	mPTXKernelFunctionNames[inKernelName] = inKernelName;
	mPTXKernelsByName[inKernelName] = ptxDefinition;
	mInputOutputTypesByName[inKernelName] = make_pair(inInputType, inOutputTypes);
	}

ImplValContainer    createFORAVector(
						uint8_t* indexData,
						uword_t  indexAlignedSize, // we can also pass the Type if necessary
						const std::vector<Type>& outputElementTypes,
						const std::vector<uint8_t*> alignedDataVectors,
						uint64_t count,
						MemoryPool* inPool
						)
	{
	if (!count || outputElementTypes.size() == 0 || alignedDataVectors.size() == 0)
		return ImplValContainerUtilities::createVector(VectorRecord());

	lassert(inPool);

	TypedFora::Abi::ForaValueArray* array =
		TypedFora::Abi::ForaValueArray::Empty(inPool);

	std::set<JOV> jovSet;
	std::vector<uint8_t*> slots(count);
	std::vector<JOV> jovs(count);
	if (indexData)
		{
		for (uword_t k = 0; k < count; ++k)
			{
			lassert(indexData);
			uint8_t index = indexData[k];
			if (index == GPU_EXCEPTION_INDEX)
				return ImplValContainerUtilities::createVector(VectorRecord());

			lassert_dump(index < outputElementTypes.size(),
					"index = " << (long)index << ", elmtTypes = " << outputElementTypes.size());
			const Type& t = outputElementTypes[index];
			slots[k] = alignedDataVectors[index] + (k * t.alignedSize());
			jovs[k] = JOV::OfType(t);
			jovSet.insert(jovs[k]);
			}
		}
	else
		jovSet.insert(JOV::OfType(outputElementTypes[0]));

	lassert(jovSet.size() > 0);
	if (jovSet.size() == 1)
		{
		const JOV& objectType = *jovSet.begin();
		uint8_t* alignedData;
		if (indexData)
			alignedData = alignedDataVectors[indexData[0]];
		else
			alignedData = alignedDataVectors[0];
		TypedFora::Abi::PackedForaValues vals = array->appendUninitialized(objectType, count);
		if (objectType.type()->alignedSize() != objectType.type()->size())
			{
			uint8_t* packedData  = vals.data();
			for (int i = 0; i < count; ++i)
				copyAlignedToPacked(*objectType.type(), (uint8_t*)alignedData, packedData);
			}
		else
			memcpy(vals.data(), alignedData, objectType.type()->size() * count);
		}
	else
		{
		TypedFora::Abi::slottedAppend(slots, jovs, jovSet, array);
		}

	VectorRecord vector(
		inPool->construct<VectorHandle>(
			Fora::BigVectorId(),
			Fora::PageletTreePtr(),
			array,
			inPool,
			ExecutionContext::currentExecutionContext()->newVectorHash()
			)
		);
	return ImplValContainerUtilities::createVector(vector);
	}


ImplValContainer	CUDAExecutionContext::executeKernel(
						const std::string&	inKernelName,
						ImplValContainer	inApplyObject,
						ImplValContainer	inSourceVector
						)
	{
	Type closureType = inApplyObject.type();
	Type inputElementType;
	const std::vector<Type>* outputElementTypes;
	bool mayThrowException = false;

		{
		boost::recursive_mutex::scoped_lock lock(mMutex);

		//verify that this kernel exists
		lassert_dump(
				mInputOutputTypesByName.find(inKernelName) != mInputOutputTypesByName.end(),
				"used kernel " << inKernelName << " without defining it."
				);
		lassert_dump(
				mMayThrowException.find(inKernelName) != mMayThrowException.end(),
				"used kernel " << inKernelName << " without defining mMayThrowException for it."
				);
		inputElementType = mInputOutputTypesByName[inKernelName].first;
		outputElementTypes = &mInputOutputTypesByName[inKernelName].second;
		mayThrowException = mMayThrowException[inKernelName];
		lassert(outputElementTypes);
		}

	lassert(inApplyObject.type().isClass());
	lassert(inSourceVector.type().isVector());

	TypedFora::Abi::VectorRecord sourceVectorHandle = inSourceVector.cast<TypedFora::Abi::VectorRecord>();

	lassert(sourceVectorHandle.size() &&
			sourceVectorHandle.jor().size() == 1 &&
			sourceVectorHandle.jor()[0].type());

	lassert_dump(
		*sourceVectorHandle.jor()[0].type() == inputElementType,
		"passed kernel " << inKernelName << " vector with elements of type "
				<< prettyPrintString(*sourceVectorHandle.jor()[0].type())
				<< " but expected" << prettyPrintString(inputElementType)
		);

	//bail if there are no elements
	if (!sourceVectorHandle.size())
		return ImplValContainer(
			CSTValue::blankOf(
				Type::Vector()
				)
			);

	uword_t elementCount = sourceVectorHandle.size();

	//create a aligned vectors
	AlignmentManager alignMgr;
	uint8_t* cudaClosureData = alignMgr.getHandleToCudaAlignedData(inApplyObject);
	uint8_t* cudaInVecData   = alignMgr.getHandleToCudaAlignedData(inSourceVector);

	if (!cudaInVecData || (!cudaClosureData && inApplyObject.type().size() > 0))
		return ImplValContainer(CSTValue::blankOf(Type::Vector()));

	uword_t outputElementTypesCount = outputElementTypes->size();
	lassert_dump(outputElementTypesCount < GPU_EXCEPTION_INDEX, // reserve value 0xFF to flag exceptions
			"Too many output types for 8bit unsigned integer to encode");
	Type indexElementType = Type::Integer(8, false);

	uint8_t* cudaOutVecIndex = nullptr;
	if (outputElementTypesCount > 1 || mayThrowException) {
		cudaOutVecIndex = alignMgr.allocateCudaAlignedData(indexElementType, elementCount);
		memset(cudaOutVecIndex, 0xff, elementCount * indexElementType.alignedSize());
	}
	std::vector<uint8_t*> cudaOutVecDataVectors(outputElementTypesCount);

	for (uword_t k = 0; k < outputElementTypesCount; ++k)
		{
		const Type& t = (*outputElementTypes)[k];
		uint8_t* ptr = alignMgr.allocateCudaAlignedData(t, elementCount);
		memset(ptr, k+1, elementCount * t.alignedSize()); // For debugging purposes
		if (!ptr)
			return ImplValContainer(CSTValue::blankOf(Type::Vector()));
		cudaOutVecDataVectors[k] = ptr;
		}

	lassert(sourceVectorHandle.dataPtr()->unpagedValues());
	lassert(sourceVectorHandle.dataPtr()->unpagedValues()->size() == elementCount);

	mCUDAState->executeFunction(
		inKernelName,
		cudaClosureData,
		elementCount,
		cudaInVecData,
		cudaOutVecDataVectors,
		cudaOutVecIndex
		);

	lassert(ExecutionContext::currentExecutionContext());
	MemoryPool* pool = ExecutionContext::currentExecutionContext()->getMemoryPool();
	auto res = createFORAVector(
					cudaOutVecIndex,
					indexElementType.alignedSize(),
					*outputElementTypes,
					cudaOutVecDataVectors,
					elementCount,
					pool);

	return res;
	}


ImplValContainer	CUDAExecutionContext::executeKernel(
						ImplValContainer	inApplyObject,
						ImplValContainer	inSourceVector
						)
	{
	Type vecElementType = *inSourceVector.cast<VectorRecord>().jor()[0].type();

	if (!vecElementType.isPOD())
		{
		throw UnableToConvertToPTX(
			prettyPrintString(vecElementType) + " isn't POD");
		}

	JudgmentOnValue	funJOV =
		JudgmentOnValue::Constant(inApplyObject.getReference()).relaxedJOV();

	string kernelName = "CUDA_" + hashToString(funJOV.hash() + vecElementType.hash());

		{
		boost::recursive_mutex::scoped_lock lock(mMutex);

		if (mNativeKernelsByName.find(kernelName) == mNativeKernelsByName.end())
			{
			ImmutableTreeVector<JudgmentOnValue> signatureJOVs =
				emptyTreeVec() +
					funJOV +
					JudgmentOnValue::Constant(CSTValue(Symbol::Call())) +
					JudgmentOnValue::OfType(vecElementType)
				;

			PolymorphicSharedPtr<Fora::SimpleForwardReasoner> reasoner(
				new Fora::SimpleForwardReasoner(
					Runtime::getRuntime().getTypedForaCompiler(),
		            Runtime::getRuntime().getInstructionGraph(),
		            Runtime::getRuntime().getAxioms()
		            )
				);

			auto frame = reasoner->reasonAboutApply(JOVT::Unnamed(signatureJOVs));

			if (frame->exits().resultPart().size() < 1 )
				{
				ostringstream msg;
				msg << "Code returns zero types ("
						<< frame->exits().resultPart().size() << ").";
				throw UnableToConvertToPTX(msg.str());
				}

			auto res = reasoner->compileEntrypointForApply(JOVT::Unnamed(signatureJOVs));

			if (!res)
				throw UnableToConvertToPTX(
					"Reasoning failed to converge."
					);

			//the target instruction might have different JOVS
			//than the signature, because the axiom might be
			//a 'class apply' in which case the instruction
			//will have all of the class object's members
			//as arguments as well. These will all be constants
			//in this case, and will be removed
			//from the NativeCFG because they're all passed as
			//'none'
			ImmutableTreeVector<JudgmentOnValue>
				targetInstructionJOVs = signatureJOVs;

			//if the joa() has multiple exit points that are compatible,
			//we need to wrap them up into a single typed value
			bool	needsReturnTypeModification = false;
			TypedFora::Converter converter;

			NativeCFG cfg =
					converter.convertCallable(
							Runtime::getRuntime().getTypedForaCompiler()->getDefinition(res->second));

			lassert(res->first == TypedFora::BlockID::entry());

			cfg = NativeCFGTransforms::convertForGpuExecution(cfg, frame->exits().resultPart().size());

			while (cfg.externalBranches().size())
				{
				string nameOfSubbranch = cfg.externalBranches()[0];
				if (Runtime::getRuntime().getTypedForaCompiler()->
						getMutuallyRecursiveFunctions(nameOfSubbranch).size())
					{
					throw UnableToConvertToPTX("contains recursion");
					}
				NativeCFG cfgToInline =
						converter.convertCallable(
								Runtime::getRuntime().getTypedForaCompiler()
									->getDefinition(nameOfSubbranch));
				cfg = NativeCFGTransforms::inlineCFG(cfg,
					NativeCFGTransforms::convertForGpuExecution(
						cfgToInline,
						cfgToInline.returnTypes().size()
						),
					nameOfSubbranch
					);
				}

			try {
				mMayThrowException[kernelName] = true;
				//a lot simpler when all variables are unique
				cfg = NativeCFGTransforms::optimize(cfg, Runtime::getRuntime().getConfig());
				cfg = NativeCFGTransforms::renameVariables(cfg);

				std::vector<Type> outputTypes(frame->exits().resultPart().size());
					{
					long k = 0;
					for (auto& jov : frame->exits().resultPart().vals())
						{
						Nullable<Type> t = jov.type();
						lassert_dump(
								t,
								"return type at index " << k << " of CUDA kernel '"
									<< kernelName << "' is missing");
						outputTypes[k] = *t;
						++k;
						}
					}
				define(kernelName, cfg, vecElementType, outputTypes);
				}
			catch(std::logic_error e)
				{
				throw UnableToConvertToPTX("internal error: " +
					string(e.what()));
				}
			}
		}
	return executeKernel(kernelName, inApplyObject, inSourceVector);
	}
