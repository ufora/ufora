/***************************************************************************
   Copyright 2015 Ufora Inc.

   Licensed under the Apache License, Version 2.0 (the "License");
   you may not use this file except in compliance with the License.
   You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

   Unless required by applicable law or agreed to in writing, software
   distributed under the License is distributed on an "AS IS" BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   See the License for the specific language governing permissions and
   limitations under the License.
****************************************************************************/
#include "VectorPages.hppml"
#include "VectorPage.hppml"
#include "../../core/Logging.hpp"
#include "../../core/PolymorphicSharedPtrBinder.hpp"
#include "../Core/ExecutionContextImpl.hppml"
#include "../../core/cppml/CPPMLPrettyPrinterUnorderedContainers.hppml"

typedef MappableMemoryPool::UnmapState UnmapState;

VectorPages::VectorPages(PolymorphicSharedPtr<CallbackScheduler> inGarbageCollectionCallbackScheduler) : 
		mBytesInUnreferencedPages(0),
		mBytesInReferencedPages(0),
		mBytesInPagesPendingDrop(0),
		mWaitingForAllVectorPagesToBeUnmapped(false),
		mGarbageCollectionCallbackScheduler(inGarbageCollectionCallbackScheduler)
	{
	}

VectorPages::~VectorPages()
	{
	}

boost::shared_ptr<VectorPage> VectorPages::getVectorPage(Fora::PageId inPage)
	{
	boost::recursive_mutex::scoped_lock lock(mMutex);

	auto it = mVectorPages.find(inPage);

	if (it != mVectorPages.end())
		return it->second;

	return boost::shared_ptr<VectorPage>();
	}

void VectorPages::triggerUnmapOfAllVectorPages()
	{
	boost::recursive_mutex::scoped_lock lock(mMutex);

	if ((mBytesInReferencedPages > 0 || mPagesToDropWhenFullyUnreferenced.size())
				&& !mWaitingForAllVectorPagesToBeUnmapped)
		{
		mWaitingForAllVectorPagesToBeUnmapped = true;

		LOG_DEBUG << (void*)this << " triggering unmap: we have " 
			<< mBytesInReferencedPages / 1024 / 1024.0 << " MB in referenced pages. we have "
			<< mPagesToDropWhenFullyUnreferenced.size() << " pages to drop when unreferenced, "
			<< mMappingStates.getKeys(UnmapState::UnmappedAndUnmappable).size() << " UnmappedAndUnmappable, "
			<< mMappingStates.getKeys(UnmapState::UnmappedAndMappable).size() << " UnmappedAndMappable, "
			<< mMappingStates.getKeys(UnmapState::Mapped).size() << " Mapped, "
			<< mMappingStates.getKeys(UnmapState::MappedAndWantsToBeUnmapped).size() << " MappedAndWantsToBeUnmapped. "
			;

		for (auto pageAndState: mMappingStates.getKeyToValue())
			pageAndState.first->markWantsToBeUnmapped();
		}
	}

bool VectorPages::isWaitingForAllVectorPagesToUnmap()
	{
	boost::recursive_mutex::scoped_lock lock(mMutex);

	return mWaitingForAllVectorPagesToBeUnmapped;
	}

void VectorPages::blockUntilAllVectorPagesAreUnmapped()
	{
	boost::recursive_mutex::scoped_lock lock(mMutex);

	LOG_INFO << (void*)this << " -- blocking until all pages unmapped";

	triggerUnmapOfAllVectorPages();

	while (mWaitingForAllVectorPagesToBeUnmapped && !areNoPagesMapped_())
		mNoPagesAreMappedCondition.wait(lock);

	LOG_INFO << (void*)this << " -- page unmap is complete";
	}

void VectorPages::allowVectorPagesToBeMappedAgain()
	{
	boost::recursive_mutex::scoped_lock lock(mMutex);

	if (!mWaitingForAllVectorPagesToBeUnmapped)
		return;

	lassert(areNoPagesMapped_());
	
	mWaitingForAllVectorPagesToBeUnmapped = false;

	for (auto pageAndState: mMappingStates.getKeyToValue())
		pageAndState.first->markMappable();
	}

void VectorPages::handleStateChangeStatic(
			PolymorphicSharedWeakPtr<VectorPages> vecPagesWeak,
			boost::weak_ptr<VectorPage> pageWeak,
			UnmapState state,
			UnmapState newState
			)
	{
	PolymorphicSharedPtr<VectorPages> pages = vecPagesWeak.lock();
	boost::shared_ptr<VectorPage> page = pageWeak.lock();

	if (pages && page)
		pages->handleStateChange(page, state, newState);
	}

void VectorPages::addVectorPage(boost::shared_ptr<VectorPage> inPage)
	{
	boost::recursive_mutex::scoped_lock lock(mMutex);

	Fora::PageId pageId = inPage->getPageId();

	lassert(!hasPage(pageId));

	mVectorPages[pageId] = inPage;

	mPageSizes[pageId] = inPage->totalBytesAllocatedFromOS();

	UnmapState currentState = inPage->subscribeToStateChanges(
		boost::bind(
			handleStateChangeStatic,
			PolymorphicSharedWeakPtr<VectorPages>(this->polymorphicSharedPtrFromThis()),
			boost::weak_ptr<VectorPage>(inPage),
			boost::arg<1>(),
			boost::arg<2>()
			)
		);

	mMappingStates.set(inPage, currentState);

	if (currentState == UnmapState::UnmappedAndMappable || 
			currentState == UnmapState::UnmappedAndUnmappable)
		{
		mBytesInUnreferencedPages += mPageSizes[pageId];
		mUnreferencedPageIds.insert(pageId);
		}
	else
		{
		mBytesInReferencedPages += mPageSizes[pageId];
		mReferencedPageIds.insert(pageId);
		}

	if (mWaitingForAllVectorPagesToBeUnmapped)
		inPage->markWantsToBeUnmapped();
		else
	if (currentState == UnmapState::UnmappedAndUnmappable)
		inPage->markMappable();

	if (!inPage->hasHadSmallPageletsCollapsed())
		mGarbageCollectionCallbackScheduler->scheduleImmediately(
			boost::bind(
				PolymorphicSharedPtrBinder::memberFunctionToWeakPtrFunction(
					&VectorPages::checkCollapseSmallPagelets
					),
				this->polymorphicSharedWeakPtrFromThis(),
				pageId
				),
			"VectorPages::checkCollapseSmallPagelets"
			);


	touchPage_(pageId);

	LOG_DEBUG << (void*)this << " on " << prettyPrintString(mMachineId)
		<< ", added page " << prettyPrintString(pageId) << " to VectorPages";

	triggerNotificationIfNecessary_();
	}

void VectorPages::vectorPageSizeChanged(Fora::PageId pageId, int64_t newSizeInBytes)
	{
	boost::recursive_mutex::scoped_lock lock(mMutex);

	if (mPageSizes.find(pageId) == mPageSizes.end())
		{
		LOG_WARN << "Pagesize changed for " << pageId << " which we don't have a record of.";
		return;
		}

	int64_t oldSize = mPageSizes[pageId];
	mPageSizes[pageId] = newSizeInBytes;

	if (mReferencedPageIds.find(pageId) != mReferencedPageIds.end())
		mBytesInReferencedPages += newSizeInBytes - oldSize;
		else
	if (mUnreferencedPageIds.find(pageId) != mUnreferencedPageIds.end())
		mBytesInUnreferencedPages += newSizeInBytes - oldSize;
	else
		mBytesInPagesPendingDrop += newSizeInBytes - oldSize;
	}

void VectorPages::checkCollapseSmallPagelets(Fora::PageId pageId)
	{
	LOG_DEBUG << (void*)this << " on " << prettyPrintString(mMachineId)
		<< ", collapsing pagelets for " << prettyPrintString(pageId);

	boost::shared_ptr<VectorPage> pagePtr;

		{
		boost::recursive_mutex::scoped_lock lock(mMutex);

		if (mWaitingForAllVectorPagesToBeUnmapped)
			return;

		if (mVectorPages.find(pageId) == mVectorPages.end())
			return;

		pagePtr = mVectorPages[pageId];
		}

	if (pagePtr->hasHadSmallPageletsCollapsed())
		return;

	if (!pagePtr->acquireGarbageCollectionLock())
		return;

	try {
		int64_t oldSize = pagePtr->totalBytesAllocatedFromOS();

		pagePtr->collapseSmallPagelets();

		int64_t newSize = pagePtr->totalBytesAllocatedFromOS();

		if (newSize > oldSize)
			LOG_WARN << "Size of a vector page increased from " 
				<< oldSize / 1024 /1024.0 
				<< " to " 
				<< newSize / 1024/1024.0 << " MB"
				;
		
		vectorPageSizeChanged(pageId, pagePtr->totalBytesAllocatedFromOS());

		pagePtr->releaseGarbageCollectionLock();
		}
	catch(...)
		{
		pagePtr->releaseGarbageCollectionLock();

		LOG_CRITICAL << "Threw an exception while collapsing pagelets!";
		throw;
		}

	}

void VectorPages::triggerNotificationIfNecessary_()
	{
	if (areNoPagesMapped_())
		mNoPagesAreMappedCondition.notify_all();
	}

bool VectorPages::areNoPagesMapped_() const
	{
	return mPagesToDropWhenFullyUnreferenced.size() == 0 && 
		mMappingStates.getKeys(UnmapState::UnmappedAndUnmappable).size() == mMappingStates.size();
	}

bool VectorPages::hasPage(Fora::PageId inPage)
	{
	boost::recursive_mutex::scoped_lock lock(mMutex);

	auto it = mVectorPages.find(inPage);

	return it != mVectorPages.end();
	}

std::set<Fora::PageId> VectorPages::copyOfAllPageIds() const
	{
	boost::recursive_mutex::scoped_lock lock(mMutex);

	std::set<Fora::PageId> result;

	for (auto pageIdAndPtr: mVectorPages)
		result.insert(pageIdAndPtr.first);

	return result;
	}

boost::shared_ptr<VectorPage> VectorPages::dropVectorPage(Fora::PageId pageId)
	{
	boost::recursive_mutex::scoped_lock lock(mMutex);

	LOG_DEBUG << "Dropping vector page " << pageId;

	auto it = mVectorPages.find(pageId);

	lassert(it != mVectorPages.end());

	boost::shared_ptr<VectorPage> page = it->second;

	lassert(page);

	mVectorPages.erase(pageId);

	mPageTouchTimes.drop(pageId);

	if (mUnreferencedPageIds.find(pageId) != mUnreferencedPageIds.end())
		{
		mUnreferencedPageIds.erase(pageId);
		mBytesInUnreferencedPages -= mPageSizes[pageId];
		}

	if (mReferencedPageIds.find(pageId) != mReferencedPageIds.end())
		{
		mReferencedPageIds.erase(pageId);
		mBytesInReferencedPages -= mPageSizes[pageId];
		}

	if (mMappingStates.getValue(page) == UnmapState::UnmappedAndUnmappable)
		{
		mMappingStates.drop(page);
		mPageSizes.erase(pageId);
		}
	else
		{
		page->markWantsToBeUnmapped();

		mBytesInPagesPendingDrop += mPageSizes[pageId];
		
		mPagesToDropWhenFullyUnreferenced.insert(page);
		}

	return page;
	}

bool VectorPages::hasPagesToDropWhenFullyUnreferenced()
	{
	boost::recursive_mutex::scoped_lock lock(mMutex);

	return mPagesToDropWhenFullyUnreferenced.size();
	}

bool VectorPages::hasUnreferencedPages() const
	{
	boost::recursive_mutex::scoped_lock lock(mMutex);

	return mUnreferencedPageIds.size() > 0;
	}

uword_t VectorPages::getBytecountOfUnreferencedPages() const
	{
	boost::recursive_mutex::scoped_lock lock(mMutex);

	return mBytesInUnreferencedPages;
	}

size_t VectorPages::getCountOfUnreferencedPages() const
	{
	boost::recursive_mutex::scoped_lock lock(mMutex);

	return mUnreferencedPageIds.size();
	}

uword_t VectorPages::getBytesInReferencedPages() const
	{
	boost::recursive_mutex::scoped_lock lock(mMutex);

	return mBytesInReferencedPages;
	}

size_t VectorPages::getCountOfReferencedPages() const
	{
	boost::recursive_mutex::scoped_lock lock(mMutex);

	return mVectorPages.size() - mUnreferencedPageIds.size() + mPagesToDropWhenFullyUnreferenced.size();
	}

size_t VectorPages::getCountOfReferencedPagesExcludingUndroppedPages() const
	{
	boost::recursive_mutex::scoped_lock lock(mMutex);

	return mVectorPages.size() - mUnreferencedPageIds.size();
	}

uword_t VectorPages::getTotalBytesUsed() const
	{
	boost::recursive_mutex::scoped_lock lock(mMutex);

	return mBytesInUnreferencedPages + mBytesInReferencedPages;
	}

void VectorPages::touchPage_(Fora::PageId inPage)
	{
	mPageTouchTimes.set(inPage, curClock());
	}

Nullable<Fora::PageId> VectorPages::oldestUnmappedPage() const
	{
	boost::recursive_mutex::scoped_lock lock(mMutex);
	
	for (auto touchTimeAndPages: mPageTouchTimes.getValueToKeys())
		{
		for (auto page: touchTimeAndPages.second)
			if (mUnreferencedPageIds.find(page) != mUnreferencedPageIds.end())
				return null() << page;
		}

	return null();
	}

size_t VectorPages::bytesOfPagesToDropWhenFullyUnreferenced() const
	{
	return mBytesInPagesPendingDrop;
	}

namespace {

bool isMappableState(MappableMemoryPool::UnmapState state)
	{
	return state == UnmapState::UnmappedAndMappable || state == UnmapState::UnmappedAndUnmappable;
	}

}

void VectorPages::handleStateChange(
			boost::shared_ptr<VectorPage> inPage, 
			MappableMemoryPool::UnmapState oldState, 
			MappableMemoryPool::UnmapState newState
			)
	{
	boost::recursive_mutex::scoped_lock lock(mMutex);

	if (mVectorPages.find(inPage->getPageId()) == mVectorPages.end() && 
			mPagesToDropWhenFullyUnreferenced.find(inPage) == mPagesToDropWhenFullyUnreferenced.end())
		{
		LOG_WARN << (void*)this << " ignoring state change of " << inPage->getPageId() << " from " 
			<< oldState << " to " << newState;

		return;
		}

	LOG_DEBUG << (void*)this << " observing state change of " << inPage->getPageId() << " from " 
		<< oldState << " to " << newState;

	lassert(mMappingStates.getValue(inPage) == oldState);
	mMappingStates.set(inPage, newState);

	//state change when we want to drop this page is simple
	if (mPagesToDropWhenFullyUnreferenced.find(inPage) != 
						mPagesToDropWhenFullyUnreferenced.end())
		{
		if (newState == UnmapState::UnmappedAndUnmappable)
			{
			mBytesInPagesPendingDrop -= mPageSizes[inPage->getPageId()];
			mPagesToDropWhenFullyUnreferenced.erase(inPage);
			mMappingStates.drop(inPage);
			mPageSizes.erase(inPage->getPageId());
			}
		}
	else
		{
		if (!isMappableState(oldState) && isMappableState(newState))
			{
			//now unmapped
			mBytesInUnreferencedPages += mPageSizes[inPage->getPageId()];
			mBytesInReferencedPages -= mPageSizes[inPage->getPageId()];

			lassert(mUnreferencedPageIds.find(inPage->getPageId()) == mUnreferencedPageIds.end());
			lassert(mReferencedPageIds.find(inPage->getPageId()) != mReferencedPageIds.end());
			
			mUnreferencedPageIds.insert(inPage->getPageId());
			mReferencedPageIds.erase(inPage->getPageId());
			}

		if (isMappableState(oldState) && !isMappableState(newState))
			{
			//now mapped
			mBytesInUnreferencedPages -= mPageSizes[inPage->getPageId()];
			mBytesInReferencedPages += mPageSizes[inPage->getPageId()];

			lassert(mUnreferencedPageIds.find(inPage->getPageId()) != mUnreferencedPageIds.end());
			lassert(mReferencedPageIds.find(inPage->getPageId()) == mReferencedPageIds.end());
			
			mReferencedPageIds.insert(inPage->getPageId());
			mUnreferencedPageIds.erase(inPage->getPageId());

			touchPage_(inPage->getPageId());
			}

		if (newState == UnmapState::UnmappedAndUnmappable && !mWaitingForAllVectorPagesToBeUnmapped)
			inPage->markMappable();

		if (newState == UnmapState::UnmappedAndUnmappable)
			{
			if (!inPage->hasHadSmallPageletsCollapsed())
				mGarbageCollectionCallbackScheduler->scheduleImmediately(
					boost::bind(
						PolymorphicSharedPtrBinder::memberFunctionToWeakPtrFunction(
							&VectorPages::checkCollapseSmallPagelets
							),
						this->polymorphicSharedWeakPtrFromThis(),
						inPage->getPageId()
						),
					"VectorPages::checkCollapseSmallPagelets"
					);
			}
		}

	triggerNotificationIfNecessary_();
	}


